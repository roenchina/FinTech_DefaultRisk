{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Read Data\n",
    "### 读取原始数据application_train/test.csv，处理完之后保存为processed_train_test.csv。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# File system manangement\n",
    "import os\n",
    "\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# modeling \n",
    "import lightgbm as lgb\n",
    "\n",
    "# utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# memory management\n",
    "import gc\n",
    "\n",
    "FILE_NAME = \"vis.ipynb\"\n",
    "PARENT_DIR = os.path.abspath(os.path.join(os.path.dirname(FILE_NAME), \".\"))\n",
    "\n",
    "app_train = pd.read_csv( PARENT_DIR + '/data/application_train.csv')\n",
    "app_test = pd.read_csv( PARENT_DIR + '/data/application_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the anomalous values with nan\n",
    "app_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
    "app_test['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate New Features\n",
    "## Polynomial Features\n",
    "### 提取兴趣特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new dataframe for polynomial features\n",
    "poly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\n",
    "poly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简单的缺失值处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputer for handling missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy = 'median')\n",
    "\n",
    "poly_target = poly_features['TARGET']\n",
    "poly_features = poly_features.drop(columns = ['TARGET'])\n",
    "\n",
    "# Need to impute missing values\n",
    "poly_features = imputer.fit_transform(poly_features)\n",
    "poly_features_test = imputer.transform(poly_features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 直接构造多项式特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "                                  \n",
    "# Create the polynomial object with specified degree\n",
    "poly_transformer = PolynomialFeatures(degree = 3)\n",
    "\n",
    "# Train the polynomial features\n",
    "poly_transformer.fit(poly_features)\n",
    "\n",
    "# Transform the features\n",
    "poly_features = poly_transformer.transform(poly_features)\n",
    "poly_features_test = poly_transformer.transform(poly_features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新特征可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dataframe of the features \n",
    "poly_features = pd.DataFrame(poly_features, \n",
    "                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n",
    "                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n",
    "\n",
    "# Add in the target\n",
    "poly_features['TARGET'] = poly_target\n",
    "\n",
    "# Find the correlations with the target\n",
    "poly_corrs = poly_features.corr()['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_corrs = poly_corrs.drop(['TARGET']).drop(['1']).abs().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display\n",
    "plt.figure(figsize = (10, 10))\n",
    "plt.bar( x=0, bottom=p_corrs.index.astype(str), height=0.25, width=p_corrs.values, orientation=\"horizontal\")\n",
    "plt.title('New Feature Correlations with target');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 20))\n",
    "# iterate through the new features\n",
    "for i, feature in enumerate(['EXT_SOURCE_2 EXT_SOURCE_3', 'EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3', 'EXT_SOURCE_2 EXT_SOURCE_3 DAYS_BIRTH', 'EXT_SOURCE_2 DAYS_BIRTH']):\n",
    "    \n",
    "    # create a new subplot for each source\n",
    "    plt.subplot(4, 1, i + 1)\n",
    "    # plot repaid loans\n",
    "    sns.kdeplot(poly_features.loc[poly_features['TARGET'] == 0, feature], label = 'target == 0')\n",
    "    # plot loans that were not repaid\n",
    "    sns.kdeplot(poly_features.loc[poly_features['TARGET'] == 1, feature], label = 'target == 1')\n",
    "    \n",
    "    # Label the plots\n",
    "    plt.title('Distribution of %s by Target Value' % feature)\n",
    "    plt.xlabel('%s' % feature); plt.ylabel('Density');\n",
    "    \n",
    "plt.tight_layout(h_pad = 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge到原数据集中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put test features into dataframe\n",
    "poly_features_test = pd.DataFrame(poly_features_test, \n",
    "                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']))\n",
    "\n",
    "# Select the best ones\n",
    "best_poly_feature_names = p_corrs.tail(10).index.to_list()\n",
    "best_poly_features = poly_features[best_poly_feature_names]\n",
    "best_poly_features_test = poly_features_test[best_poly_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training data with polynomial features shape:  (307511, 132)\nTesting data with polynomial features shape:   (48744, 131)\n"
    }
   ],
   "source": [
    "# Merge polynomial features into training dataframe\n",
    "best_poly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\n",
    "app_train_poly = app_train.merge(best_poly_features, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "# Merge polnomial features into testing dataframe\n",
    "best_poly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\n",
    "app_test_poly = app_test.merge(best_poly_features_test, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "# Align the dataframes\n",
    "app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n",
    "\n",
    "# Add the target column to train dataset\n",
    "app_train_poly['TARGET'] = poly_target\n",
    "\n",
    "# Print out the new shapes\n",
    "print('Training data with polynomial features shape: ', app_train_poly.shape)\n",
    "print('Testing data with polynomial features shape:  ', app_test_poly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Knowledge Features\n",
    "### 训练集构造特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_domain = app_train_poly.copy()\n",
    "app_test_domain = app_test_poly.copy()\n",
    "\n",
    "app_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\n",
    "app_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\n",
    "app_train_domain['ANNUITY_CREDIT_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\n",
    "app_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试集构造特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\n",
    "app_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\n",
    "app_test_domain['ANNUITY_CREDIT_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\n",
    "app_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training data with polynomial & domain features shape:  (307511, 136)\nTesting data with polynomial $ domain features shape:   (48744, 135)\n"
    }
   ],
   "source": [
    "# Print out the new shapes\n",
    "print('Training data with polynomial & domain features shape: ', app_train_domain.shape)\n",
    "print('Testing data with polynomial $ domain features shape:  ', app_test_domain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征相关性可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_features = app_train_domain[['AMT_CREDIT', 'AMT_INCOME_TOTAL', 'AMT_ANNUITY', 'DAYS_EMPLOYED', 'DAYS_BIRTH', 'CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'ANNUITY_CREDIT_PERCENT', 'DAYS_EMPLOYED_PERCENT', 'TARGET']]\n",
    "domain_corrs = domain_features.corr()['TARGET'].sort_values()\n",
    "domain_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display most relevant\n",
    "d_corrs = domain_corrs.drop(['TARGET']).abs().sort_values()\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.bar( x=0, bottom=d_corrs.index.astype(str), height=0.5, width=d_corrs.values, orientation=\"horizontal\")\n",
    "plt.title('New Feature Correlations with target');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 20))\n",
    "# iterate through the new features\n",
    "for i, feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'ANNUITY_CREDIT_PERCENT', 'DAYS_EMPLOYED_PERCENT']):\n",
    "    \n",
    "    # create a new subplot for each source\n",
    "    plt.subplot(4, 1, i + 1)\n",
    "    # plot repaid loans\n",
    "    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 0, feature], label = 'target == 0')\n",
    "    # plot loans that were not repaid\n",
    "    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 1, feature], label = 'target == 1')\n",
    "    \n",
    "    # Label the plots\n",
    "    plt.title('Distribution of %s by Target Value' % feature)\n",
    "    plt.xlabel('%s' % feature); plt.ylabel('Density');\n",
    "    \n",
    "plt.tight_layout(h_pad = 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_auto = app_train.copy()\n",
    "app_test_auto = app_test.copy()\n",
    "\n",
    "app_train_auto = app_train_auto.sort_values('SK_ID_CURR').reset_index(drop = True).loc[:1000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import featuretools as ft\n",
    "\n",
    "# Entity set with id applications\n",
    "es = ft.EntitySet(id = 'clients')\n",
    "\n",
    "# Entities with a unique index\n",
    "es = es.entity_from_dataframe(entity_id = 'app_train', dataframe = app_train_auto, index = 'SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default primitives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primitives = ft.list_primitives()\n",
    "pd.options.display.max_colwidth = 100\n",
    "primitives[primitives['type'] == 'transform']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Default primitives from featuretools\n",
    "default_agg_primitives = []\n",
    "default_trans_primitives =  [\"diff\", \"divide_by_feature\", \"absolute\", \"haversine\"]\n",
    "\n",
    "# DFS with specified primitives\n",
    "feature_matrix, feature_names = ft.dfs(entityset = es, target_entity = 'app_train',\n",
    "                    trans_primitives = default_trans_primitives,\n",
    "                    agg_primitives = default_agg_primitives,\n",
    "                    max_depth = 2, features_only = False, verbose = True)\n",
    "\n",
    "print('%d Total Features' % len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result datasets with new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_nf = app_train_domain.copy()\n",
    "app_test_nf = app_test_domain.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "## Remove Collinear Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                  SK_ID_CURR  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  \\\nSK_ID_CURR               NaN      0.001129          0.001820    0.000343   \nCNT_CHILDREN             NaN           NaN          0.012882    0.002145   \nAMT_INCOME_TOTAL         NaN           NaN               NaN    0.156870   \nAMT_CREDIT               NaN           NaN               NaN         NaN   \nAMT_ANNUITY              NaN           NaN               NaN         NaN   \n\n                  AMT_ANNUITY  AMT_GOODS_PRICE  REGION_POPULATION_RELATIVE  \\\nSK_ID_CURR           0.000433         0.000232                    0.000849   \nCNT_CHILDREN         0.021374         0.001827                    0.025573   \nAMT_INCOME_TOTAL     0.191657         0.159610                    0.074796   \nAMT_CREDIT           0.770138         0.986968                    0.099738   \nAMT_ANNUITY               NaN         0.775109                    0.118429   \n\n                  DAYS_BIRTH  DAYS_EMPLOYED  DAYS_REGISTRATION  ...  \\\nSK_ID_CURR          0.001500       0.000084           0.000973  ...   \nCNT_CHILDREN        0.330938       0.061145           0.183395  ...   \nAMT_INCOME_TOTAL    0.027261       0.013005           0.027805  ...   \nAMT_CREDIT          0.055436       0.091295           0.009621  ...   \nAMT_ANNUITY         0.009445       0.053604           0.038514  ...   \n\n                  EXT_SOURCE_2 EXT_SOURCE_3^2  EXT_SOURCE_2^2 EXT_SOURCE_3  \\\nSK_ID_CURR                           0.001338                     0.001527   \nCNT_CHILDREN                         0.039596                     0.035572   \nAMT_INCOME_TOTAL                     0.008854                     0.039285   \nAMT_CREDIT                           0.090770                     0.127608   \nAMT_ANNUITY                          0.083815                     0.120899   \n\n                  EXT_SOURCE_2 EXT_SOURCE_3 DAYS_BIRTH  \\\nSK_ID_CURR                                    0.001149   \nCNT_CHILDREN                                  0.175433   \nAMT_INCOME_TOTAL                              0.006438   \nAMT_CREDIT                                    0.107749   \nAMT_ANNUITY                                   0.078002   \n\n                  EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3  \\\nSK_ID_CURR                                      0.001189   \nCNT_CHILDREN                                    0.073692   \nAMT_INCOME_TOTAL                                0.031090   \nAMT_CREDIT                                      0.141562   \nAMT_ANNUITY                                     0.123227   \n\n                  EXT_SOURCE_2 EXT_SOURCE_3    TARGET  CREDIT_INCOME_PERCENT  \\\nSK_ID_CURR                         0.001658  0.002108               0.001726   \nCNT_CHILDREN                       0.037726  0.019187               0.016012   \nAMT_INCOME_TOTAL                   0.023917  0.003982               0.108191   \nAMT_CREDIT                         0.113471  0.030369               0.651097   \nAMT_ANNUITY                        0.105109  0.012817               0.393239   \n\n                  ANNUITY_INCOME_PERCENT  ANNUITY_CREDIT_PERCENT  \\\nSK_ID_CURR                      0.002351                0.000319   \nCNT_CHILDREN                    0.002296                0.020751   \nAMT_INCOME_TOTAL                0.153033                0.026788   \nAMT_CREDIT                      0.373921                0.558789   \nAMT_ANNUITY                     0.484624                0.063489   \n\n                  DAYS_EMPLOYED_PERCENT  \nSK_ID_CURR                     0.000136  \nCNT_CHILDREN                   0.009985  \nAMT_INCOME_TOTAL               0.010678  \nAMT_CREDIT                     0.066800  \nAMT_ANNUITY                    0.041393  \n\n[5 rows x 120 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SK_ID_CURR</th>\n      <th>CNT_CHILDREN</th>\n      <th>AMT_INCOME_TOTAL</th>\n      <th>AMT_CREDIT</th>\n      <th>AMT_ANNUITY</th>\n      <th>AMT_GOODS_PRICE</th>\n      <th>REGION_POPULATION_RELATIVE</th>\n      <th>DAYS_BIRTH</th>\n      <th>DAYS_EMPLOYED</th>\n      <th>DAYS_REGISTRATION</th>\n      <th>...</th>\n      <th>EXT_SOURCE_2 EXT_SOURCE_3^2</th>\n      <th>EXT_SOURCE_2^2 EXT_SOURCE_3</th>\n      <th>EXT_SOURCE_2 EXT_SOURCE_3 DAYS_BIRTH</th>\n      <th>EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3</th>\n      <th>EXT_SOURCE_2 EXT_SOURCE_3</th>\n      <th>TARGET</th>\n      <th>CREDIT_INCOME_PERCENT</th>\n      <th>ANNUITY_INCOME_PERCENT</th>\n      <th>ANNUITY_CREDIT_PERCENT</th>\n      <th>DAYS_EMPLOYED_PERCENT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>SK_ID_CURR</th>\n      <td>NaN</td>\n      <td>0.001129</td>\n      <td>0.001820</td>\n      <td>0.000343</td>\n      <td>0.000433</td>\n      <td>0.000232</td>\n      <td>0.000849</td>\n      <td>0.001500</td>\n      <td>0.000084</td>\n      <td>0.000973</td>\n      <td>...</td>\n      <td>0.001338</td>\n      <td>0.001527</td>\n      <td>0.001149</td>\n      <td>0.001189</td>\n      <td>0.001658</td>\n      <td>0.002108</td>\n      <td>0.001726</td>\n      <td>0.002351</td>\n      <td>0.000319</td>\n      <td>0.000136</td>\n    </tr>\n    <tr>\n      <th>CNT_CHILDREN</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.012882</td>\n      <td>0.002145</td>\n      <td>0.021374</td>\n      <td>0.001827</td>\n      <td>0.025573</td>\n      <td>0.330938</td>\n      <td>0.061145</td>\n      <td>0.183395</td>\n      <td>...</td>\n      <td>0.039596</td>\n      <td>0.035572</td>\n      <td>0.175433</td>\n      <td>0.073692</td>\n      <td>0.037726</td>\n      <td>0.019187</td>\n      <td>0.016012</td>\n      <td>0.002296</td>\n      <td>0.020751</td>\n      <td>0.009985</td>\n    </tr>\n    <tr>\n      <th>AMT_INCOME_TOTAL</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.156870</td>\n      <td>0.191657</td>\n      <td>0.159610</td>\n      <td>0.074796</td>\n      <td>0.027261</td>\n      <td>0.013005</td>\n      <td>0.027805</td>\n      <td>...</td>\n      <td>0.008854</td>\n      <td>0.039285</td>\n      <td>0.006438</td>\n      <td>0.031090</td>\n      <td>0.023917</td>\n      <td>0.003982</td>\n      <td>0.108191</td>\n      <td>0.153033</td>\n      <td>0.026788</td>\n      <td>0.010678</td>\n    </tr>\n    <tr>\n      <th>AMT_CREDIT</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.770138</td>\n      <td>0.986968</td>\n      <td>0.099738</td>\n      <td>0.055436</td>\n      <td>0.091295</td>\n      <td>0.009621</td>\n      <td>...</td>\n      <td>0.090770</td>\n      <td>0.127608</td>\n      <td>0.107749</td>\n      <td>0.141562</td>\n      <td>0.113471</td>\n      <td>0.030369</td>\n      <td>0.651097</td>\n      <td>0.373921</td>\n      <td>0.558789</td>\n      <td>0.066800</td>\n    </tr>\n    <tr>\n      <th>AMT_ANNUITY</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.775109</td>\n      <td>0.118429</td>\n      <td>0.009445</td>\n      <td>0.053604</td>\n      <td>0.038514</td>\n      <td>...</td>\n      <td>0.083815</td>\n      <td>0.120899</td>\n      <td>0.078002</td>\n      <td>0.123227</td>\n      <td>0.105109</td>\n      <td>0.012817</td>\n      <td>0.393239</td>\n      <td>0.484624</td>\n      <td>0.063489</td>\n      <td>0.041393</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 120 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# Absolute value correlation matrix\n",
    "corr_matrix = app_train_nf.corr().abs()\n",
    "# Upper triangle of correlations\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "upper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "There are 16 columns to remove.\n"
    }
   ],
   "source": [
    "# Threshold for removing correlated variables\n",
    "threshold = 0.99\n",
    "\n",
    "# Select columns with correlations above threshold\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "print('There are %d columns to remove.' % (len(to_drop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training shape:  (307511, 120)\nTesting shape:  (48744, 119)\n"
    }
   ],
   "source": [
    "app_train_nf = app_train_nf.drop(columns = to_drop)\n",
    "app_test_nf = app_test_nf.drop(columns = to_drop)\n",
    "\n",
    "print('Training shape: ', app_train_nf.shape)\n",
    "print('Testing shape: ', app_test_nf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_nf.to_csv(PARENT_DIR + '/data/processed_train.csv')\n",
    "app_test_nf.to_csv(PARENT_DIR + '/data/processed_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train missing values (in percent)\n",
    "train_missing = (app_train_nf.isnull().sum() / len(app_train_nf)).sort_values(ascending = False)\n",
    "train_missing.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test missing values (in percent)\n",
    "test_missing = (app_test_nf.isnull().sum() / len(app_test_nf)).sort_values(ascending = False)\n",
    "test_missing.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify missing values above threshold\n",
    "train_missing = train_missing.index[train_missing > 0.75]\n",
    "test_missing = test_missing.index[test_missing > 0.75]\n",
    "\n",
    "all_missing = list(set(set(train_missing) | set(test_missing)))\n",
    "print('There are %d columns with more than 75%% missing values' % len(all_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_nf = pd.get_dummies(app_train_nf.drop(columns = all_missing))\n",
    "app_test_nf = pd.get_dummies(app_test_nf.drop(columns = all_missing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove features with lower importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# some pre\n",
    "# Need to save the labels because aligning will remove this column\n",
    "train_labels = app_train_nf[\"TARGET\"]\n",
    "train_ids = app_train_nf['SK_ID_CURR']\n",
    "test_ids = app_test_nf['SK_ID_CURR']\n",
    "\n",
    "app_train_nf = pd.get_dummies(app_train_nf.drop(columns = all_missing))\n",
    "app_test_nf = pd.get_dummies(app_test_nf.drop(columns = all_missing))\n",
    "\n",
    "app_train_nf, app_test_nf = app_train_nf.align(app_test_nf, join = 'inner', axis = 1)\n",
    "\n",
    "print('Training set full shape: ', app_train_nf.shape)\n",
    "print('Testing set full shape: ' , app_test_nf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_nf = app_train_nf.drop(columns = ['SK_ID_CURR'])\n",
    "app_test_nf = app_test_nf.drop(columns = ['SK_ID_CURR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer2 = SimpleImputer(strategy = 'median')\n",
    "# Need to impute missing values\n",
    "app_train_nf = imputer2.fit_transform(app_train_nf)\n",
    "app_train_nf = pd.DataFrame(app_train_nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty array to hold feature importances\n",
    "feature_importances = np.zeros(app_train_nf.shape[1])\n",
    "\n",
    "# Create the model with several hyperparameters\n",
    "model = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 10000, class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit the model twice to avoid overfitting\n",
    "for i in range(2):\n",
    "    \n",
    "    # Split into training and validation set\n",
    "    train_features, valid_features, train_y, valid_y = train_test_split(app_train_nf, train_labels, test_size = 0.25, random_state = i)\n",
    "    \n",
    "    # Train using early stopping\n",
    "    model.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, valid_y)], \n",
    "              eval_metric = 'auc', verbose = 200)\n",
    "    \n",
    "    # Record the feature importances\n",
    "    feature_importances += model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to average feature importances! \n",
    "feature_importances = feature_importances / 2\n",
    "feature_importances = pd.DataFrame({'feature': list(app_train_nf.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\n",
    "\n",
    "feature_importances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find the features with zero importance\n",
    "zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\n",
    "print('There are %d features with 0.0 importance' % len(zero_features))\n",
    "feature_importances.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(df, threshold = 0.9):\n",
    "    \"\"\"\n",
    "    Plots 15 most important features and the cumulative importance of features.\n",
    "    Prints the number of features needed to reach threshold cumulative importance.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    df : dataframe\n",
    "        Dataframe of feature importances. Columns must be feature and importance\n",
    "    threshold : float, default = 0.9\n",
    "        Threshold for prining information about cumulative importances\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "    df : dataframe\n",
    "        Dataframe ordered by feature importances with a normalized column (sums to 1)\n",
    "        and a cumulative importance column\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.rcParams['font.size'] = 18\n",
    "    \n",
    "    # Sort features according to importance\n",
    "    df = df.sort_values('importance', ascending = False).reset_index()\n",
    "    \n",
    "    # Normalize the feature importances to add up to one\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n",
    "\n",
    "    # Make a horizontal bar chart of feature importances\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    ax = plt.subplot()\n",
    "    \n",
    "    # Need to reverse the index to plot most important on top\n",
    "    ax.barh(list(reversed(list(df.index[:15]))), \n",
    "            df['importance_normalized'].head(15), \n",
    "            align = 'center', edgecolor = 'k')\n",
    "    \n",
    "    # Set the yticks and labels\n",
    "    ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
    "    ax.set_yticklabels(df['feature'].head(15))\n",
    "    \n",
    "    # Plot labeling\n",
    "    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n",
    "    plt.show()\n",
    "    \n",
    "    # Cumulative importance plot\n",
    "    plt.figure(figsize = (8, 6))\n",
    "    plt.plot(list(range(len(df))), df['cumulative_importance'], 'r-')\n",
    "    plt.xlabel('Number of Features'); plt.ylabel('Cumulative Importance'); \n",
    "    plt.title('Cumulative Feature Importance');\n",
    "    plt.show();\n",
    "    \n",
    "    importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n",
    "    print('%d features required for %0.2f of cumulative importance' % (importance_index + 1, threshold))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "norm_feature_importances = plot_feature_importances(feature_importances, 0.99)"
   ]
  }
 ]
}