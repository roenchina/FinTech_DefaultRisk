{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Read Data\n",
    "### 读取原始数据application_train/test.csv，处理完之后保存为processed_train_test.csv。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# File system manangement\n",
    "import os\n",
    "\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# modeling \n",
    "import lightgbm as lgb\n",
    "\n",
    "# utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# memory management\n",
    "import gc\n",
    "\n",
    "FILE_NAME = \"vis.ipynb\"\n",
    "PARENT_DIR = os.path.abspath(os.path.join(os.path.dirname(FILE_NAME), \".\"))\n",
    "\n",
    "app_train = pd.read_csv( PARENT_DIR + '/data/application_train.csv')\n",
    "app_test = pd.read_csv( PARENT_DIR + '/data/application_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate New Features\n",
    "## Polynomial Features\n",
    "### 提取兴趣特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new dataframe for polynomial features\n",
    "poly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\n",
    "poly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简单的缺失值处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputer for handling missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy = 'median')\n",
    "\n",
    "poly_target = poly_features['TARGET']\n",
    "poly_features = poly_features.drop(columns = ['TARGET'])\n",
    "\n",
    "# Need to impute missing values\n",
    "poly_features = imputer.fit_transform(poly_features)\n",
    "poly_features_test = imputer.transform(poly_features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 直接构造多项式特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "                                  \n",
    "# Create the polynomial object with specified degree\n",
    "poly_transformer = PolynomialFeatures(degree = 3)\n",
    "\n",
    "# Train the polynomial features\n",
    "poly_transformer.fit(poly_features)\n",
    "\n",
    "# Transform the features\n",
    "poly_features = poly_transformer.transform(poly_features)\n",
    "poly_features_test = poly_transformer.transform(poly_features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新特征可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dataframe of the features \n",
    "poly_features = pd.DataFrame(poly_features, \n",
    "                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n",
    "                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n",
    "\n",
    "# Add in the target\n",
    "poly_features['TARGET'] = poly_target\n",
    "\n",
    "# Find the correlations with the target\n",
    "poly_corrs = poly_features.corr()['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_corrs = poly_corrs.drop(['TARGET']).drop(['1']).abs().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display\n",
    "plt.figure(figsize = (10, 10))\n",
    "plt.bar( x=0, bottom=p_corrs.index.astype(str), height=0.25, width=p_corrs.values, orientation=\"horizontal\")\n",
    "plt.title('New Feature Correlations with target');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 20))\n",
    "# iterate through the new features\n",
    "for i, feature in enumerate(['EXT_SOURCE_2 EXT_SOURCE_3', 'EXT_SOURCE_1 EXT_SOURCE_2 EXT_SOURCE_3', 'EXT_SOURCE_2 EXT_SOURCE_3 DAYS_BIRTH', 'EXT_SOURCE_2 DAYS_BIRTH']):\n",
    "    \n",
    "    # create a new subplot for each source\n",
    "    plt.subplot(4, 1, i + 1)\n",
    "    # plot repaid loans\n",
    "    sns.kdeplot(poly_features.loc[poly_features['TARGET'] == 0, feature], label = 'target == 0')\n",
    "    # plot loans that were not repaid\n",
    "    sns.kdeplot(poly_features.loc[poly_features['TARGET'] == 1, feature], label = 'target == 1')\n",
    "    \n",
    "    # Label the plots\n",
    "    plt.title('Distribution of %s by Target Value' % feature)\n",
    "    plt.xlabel('%s' % feature); plt.ylabel('Density');\n",
    "    \n",
    "plt.tight_layout(h_pad = 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge到原数据集中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put test features into dataframe\n",
    "poly_features_test = pd.DataFrame(poly_features_test, \n",
    "                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']))\n",
    "\n",
    "# Select the best ones\n",
    "best_poly_feature_names = p_corrs.tail(10).index.to_list()\n",
    "best_poly_features = poly_features[best_poly_feature_names]\n",
    "best_poly_features_test = poly_features_test[best_poly_feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge polynomial features into training dataframe\n",
    "best_poly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\n",
    "app_train_poly = app_train.merge(best_poly_features, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "# Merge polnomial features into testing dataframe\n",
    "best_poly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\n",
    "app_test_poly = app_test.merge(best_poly_features_test, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "# Align the dataframes\n",
    "app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n",
    "\n",
    "# Add the target column to train dataset\n",
    "app_train_poly['TARGET'] = poly_target\n",
    "\n",
    "# Print out the new shapes\n",
    "print('Training data with polynomial features shape: ', app_train_poly.shape)\n",
    "print('Testing data with polynomial features shape:  ', app_test_poly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Knowledge Features\n",
    "### 训练集构造特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_domain = app_train_poly.copy()\n",
    "app_test_domain = app_test_poly.copy()\n",
    "\n",
    "app_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\n",
    "app_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\n",
    "app_train_domain['ANNUITY_CREDIT_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\n",
    "app_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试集构造特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\n",
    "app_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\n",
    "app_test_domain['ANNUITY_CREDIT_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\n",
    "app_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print out the new shapes\n",
    "print('Training data with polynomial & domain features shape: ', app_train_domain.shape)\n",
    "print('Testing data with polynomial $ domain features shape:  ', app_test_domain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征相关性可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_features = app_train_domain[['AMT_CREDIT', 'AMT_INCOME_TOTAL', 'AMT_ANNUITY', 'DAYS_EMPLOYED', 'DAYS_BIRTH', 'CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'ANNUITY_CREDIT_PERCENT', 'DAYS_EMPLOYED_PERCENT', 'TARGET']]\n",
    "domain_corrs = domain_features.corr()['TARGET'].sort_values()\n",
    "domain_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display most relevant\n",
    "d_corrs = domain_corrs.drop(['TARGET']).abs().sort_values()\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.bar( x=0, bottom=d_corrs.index.astype(str), height=0.5, width=d_corrs.values, orientation=\"horizontal\")\n",
    "plt.title('New Feature Correlations with target');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 20))\n",
    "# iterate through the new features\n",
    "for i, feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'ANNUITY_CREDIT_PERCENT', 'DAYS_EMPLOYED_PERCENT']):\n",
    "    \n",
    "    # create a new subplot for each source\n",
    "    plt.subplot(4, 1, i + 1)\n",
    "    # plot repaid loans\n",
    "    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 0, feature], label = 'target == 0')\n",
    "    # plot loans that were not repaid\n",
    "    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 1, feature], label = 'target == 1')\n",
    "    \n",
    "    # Label the plots\n",
    "    plt.title('Distribution of %s by Target Value' % feature)\n",
    "    plt.xlabel('%s' % feature); plt.ylabel('Density');\n",
    "    \n",
    "plt.tight_layout(h_pad = 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_auto = app_train.copy()\n",
    "app_test_auto = app_test.copy()\n",
    "\n",
    "app_train_auto = app_train_auto.sort_values('SK_ID_CURR').reset_index(drop = True).loc[:1000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import featuretools as ft\n",
    "\n",
    "# Entity set with id applications\n",
    "es = ft.EntitySet(id = 'clients')\n",
    "\n",
    "# Entities with a unique index\n",
    "es = es.entity_from_dataframe(entity_id = 'app_train', dataframe = app_train_auto, index = 'SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default primitives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primitives = ft.list_primitives()\n",
    "pd.options.display.max_colwidth = 100\n",
    "primitives[primitives['type'] == 'transform']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Default primitives from featuretools\n",
    "default_agg_primitives = []\n",
    "default_trans_primitives =  [\"diff\", \"divide_by_feature\", \"absolute\", \"haversine\"]\n",
    "\n",
    "# DFS with specified primitives\n",
    "feature_matrix, feature_names = ft.dfs(entityset = es, target_entity = 'app_train',\n",
    "                    trans_primitives = default_trans_primitives,\n",
    "                    agg_primitives = default_agg_primitives,\n",
    "                    max_depth = 2, features_only = False, verbose = True)\n",
    "\n",
    "print('%d Total Features' % len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result datasets with new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_nf = app_train_domain.copy()\n",
    "app_test_nf = app_test_domain.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "## Remove Collinear Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute value correlation matrix\n",
    "corr_matrix = app_train_nf.corr().abs()\n",
    "# Upper triangle of correlations\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "upper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Threshold for removing correlated variables\n",
    "threshold = 0.99\n",
    "\n",
    "# Select columns with correlations above threshold\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "print('There are %d columns to remove.' % (len(to_drop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "app_train_nf = app_train_nf.drop(columns = to_drop)\n",
    "app_test_nf = app_test_nf.drop(columns = to_drop)\n",
    "\n",
    "print('Training shape: ', app_train_nf.shape)\n",
    "print('Testing shape: ', app_test_nf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_nf.to_csv(PARENT_DIR + '/data/processed_train.csv')\n",
    "app_test_nf.to_csv(PARENT_DIR + '/data/processed_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train missing values (in percent)\n",
    "train_missing = (app_train_nf.isnull().sum() / len(app_train_nf)).sort_values(ascending = False)\n",
    "train_missing.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test missing values (in percent)\n",
    "test_missing = (app_test_nf.isnull().sum() / len(app_test_nf)).sort_values(ascending = False)\n",
    "test_missing.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify missing values above threshold\n",
    "train_missing = train_missing.index[train_missing > 0.75]\n",
    "test_missing = test_missing.index[test_missing > 0.75]\n",
    "\n",
    "all_missing = list(set(set(train_missing) | set(test_missing)))\n",
    "print('There are %d columns with more than 75%% missing values' % len(all_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_nf = pd.get_dummies(app_train_nf.drop(columns = all_missing))\n",
    "app_test_nf = pd.get_dummies(app_test_nf.drop(columns = all_missing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove features with lower importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# some pre\n",
    "# Need to save the labels because aligning will remove this column\n",
    "train_labels = app_train_nf[\"TARGET\"]\n",
    "train_ids = app_train_nf['SK_ID_CURR']\n",
    "test_ids = app_test_nf['SK_ID_CURR']\n",
    "\n",
    "app_train_nf = pd.get_dummies(app_train_nf.drop(columns = all_missing))\n",
    "app_test_nf = pd.get_dummies(app_test_nf.drop(columns = all_missing))\n",
    "\n",
    "app_train_nf, app_test_nf = app_train_nf.align(app_test_nf, join = 'inner', axis = 1)\n",
    "\n",
    "print('Training set full shape: ', app_train_nf.shape)\n",
    "print('Testing set full shape: ' , app_test_nf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_train_nf = app_train_nf.drop(columns = ['SK_ID_CURR'])\n",
    "app_test_nf = app_test_nf.drop(columns = ['SK_ID_CURR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer2 = SimpleImputer(strategy = 'median')\n",
    "# Need to impute missing values\n",
    "app_train_nf = imputer2.fit_transform(app_train_nf)\n",
    "app_train_nf = pd.DataFrame(app_train_nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty array to hold feature importances\n",
    "feature_importances = np.zeros(app_train_nf.shape[1])\n",
    "\n",
    "# Create the model with several hyperparameters\n",
    "model = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 10000, class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit the model twice to avoid overfitting\n",
    "for i in range(2):\n",
    "    \n",
    "    # Split into training and validation set\n",
    "    train_features, valid_features, train_y, valid_y = train_test_split(app_train_nf, train_labels, test_size = 0.25, random_state = i)\n",
    "    \n",
    "    # Train using early stopping\n",
    "    model.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, valid_y)], \n",
    "              eval_metric = 'auc', verbose = 200)\n",
    "    \n",
    "    # Record the feature importances\n",
    "    feature_importances += model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to average feature importances! \n",
    "feature_importances = feature_importances / 2\n",
    "feature_importances = pd.DataFrame({'feature': list(app_train_nf.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\n",
    "\n",
    "feature_importances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find the features with zero importance\n",
    "zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\n",
    "print('There are %d features with 0.0 importance' % len(zero_features))\n",
    "feature_importances.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(df, threshold = 0.9):\n",
    "    \"\"\"\n",
    "    Plots 15 most important features and the cumulative importance of features.\n",
    "    Prints the number of features needed to reach threshold cumulative importance.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    df : dataframe\n",
    "        Dataframe of feature importances. Columns must be feature and importance\n",
    "    threshold : float, default = 0.9\n",
    "        Threshold for prining information about cumulative importances\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "    df : dataframe\n",
    "        Dataframe ordered by feature importances with a normalized column (sums to 1)\n",
    "        and a cumulative importance column\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.rcParams['font.size'] = 18\n",
    "    \n",
    "    # Sort features according to importance\n",
    "    df = df.sort_values('importance', ascending = False).reset_index()\n",
    "    \n",
    "    # Normalize the feature importances to add up to one\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n",
    "\n",
    "    # Make a horizontal bar chart of feature importances\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    ax = plt.subplot()\n",
    "    \n",
    "    # Need to reverse the index to plot most important on top\n",
    "    ax.barh(list(reversed(list(df.index[:15]))), \n",
    "            df['importance_normalized'].head(15), \n",
    "            align = 'center', edgecolor = 'k')\n",
    "    \n",
    "    # Set the yticks and labels\n",
    "    ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
    "    ax.set_yticklabels(df['feature'].head(15))\n",
    "    \n",
    "    # Plot labeling\n",
    "    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n",
    "    plt.show()\n",
    "    \n",
    "    # Cumulative importance plot\n",
    "    plt.figure(figsize = (8, 6))\n",
    "    plt.plot(list(range(len(df))), df['cumulative_importance'], 'r-')\n",
    "    plt.xlabel('Number of Features'); plt.ylabel('Cumulative Importance'); \n",
    "    plt.title('Cumulative Feature Importance');\n",
    "    plt.show();\n",
    "    \n",
    "    importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n",
    "    print('%d features required for %0.2f of cumulative importance' % (importance_index + 1, threshold))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "norm_feature_importances = plot_feature_importances(feature_importances, 0.99)"
   ]
  }
 ]
}